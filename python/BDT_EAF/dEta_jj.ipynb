{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4002-114f-485e-a39f-d614e095b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import awkward as ak\n",
    "# import dask\n",
    "import json\n",
    "# from coffea import processor\n",
    "# from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import hist\n",
    "\n",
    "\n",
    "# import mplhep as hep\n",
    "# plt.style.use([hep.style.CMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb1de3-40f1-4d21-bc80-424ff333e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('samples_noQCD2000.json', 'r') as file:\n",
    "with open('samples.json', 'r') as file:\n",
    "    pmap = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d79fe-d2ad-49e0-9976-d7265672c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''processing functions'''\n",
    "'''Cut definitions'''\n",
    "def minjetkin(df):\n",
    "    # fatjets = df['ak8FatJetmsoftdrop', 'ak8FatJetPt', 'ak8FatJetEta']\n",
    "    # print(df['ak8FatJetmsoftdrop'].shape)\n",
    "    fatjet_msd = df['FatJet0_msd'].values\n",
    "    fatjet_pt = df['FatJet0_pt'].values\n",
    "    fatjet_eta = df['FatJet0_eta'].values\n",
    "        # fatjets['msdcorr'] = fatjets.msoftdrop\n",
    "        # fatjets['qcdrho'] = 2 * np.log(fatjets.msdcorr / fatjets.pt)\n",
    "    candidatejet = df[\n",
    "            (fatjet_pt > 200)\n",
    "            & (abs(fatjet_eta) < 2.5)\n",
    "            # & fatjets.isTight \n",
    "        ]\n",
    "\n",
    "    # candidatejet = candidatejet[:, :2]\n",
    "    # candidatejet = ak.firsts(candidatejet[ak.argmax(candidatejet.particleNet_XbbVsQCD, axis=1, keepdims=True)])\n",
    "\n",
    "    # bvl = candidatejet.particleNet_XbbVsQCD\n",
    "    minjetkin=np.array([\n",
    "            (candidatejet['FatJet0_pt'] >= 450)\n",
    "            & (candidatejet['FatJet0_pt']< 1200)\n",
    "            & (candidatejet['FatJet0_msd'] >= 40.)\n",
    "            & (candidatejet['FatJet0_msd'] < 201.)\n",
    "            & (abs(candidatejet['FatJet0_eta']) < 2.5)\n",
    "       ])\n",
    "    # minjetkin=np.sum(minjetkin, axis=1).astype('bool').transpose()\n",
    "    minjetkin = minjetkin.astype('bool').transpose()\n",
    "\n",
    "    # print(minjetkin)\n",
    "    # print(minjetkin.shape)\n",
    "    # print(minjetkin)\n",
    "    \n",
    "    return df[minjetkin]\n",
    "    \n",
    "def get_paths(year, data_path, proc = 'QCD', deep=False):\n",
    "    #returns list of paths to parquet files\n",
    "    parquet_parents = [os.path.join(data_path, year, p, 'parquet','signal-all') for p in pmap[proc]]\n",
    "    \n",
    "    if deep:\n",
    "        file_list=None\n",
    "        for parent in parquet_parents:\n",
    "            if file_list is None:\n",
    "                file_list = [os.path.join(parent,file)for file in os.listdir(parent)]\n",
    "            else:\n",
    "                file_list = np.append(file_list, [os.path.join(parent,file)for file in os.listdir(parent)])\n",
    "    else:\n",
    "        file_list=parquet_parents\n",
    "    return file_list\n",
    "    \n",
    "def process_single(df, \n",
    "                   cuts=False,\n",
    "                   save_fields = ['weight','FatJet0_pt'],\n",
    "                   signal = False,\n",
    "               ):    \n",
    "    if cuts: \n",
    "        dfc = minjetkin(df.copy(deep=True))\n",
    "        #add more cuts here\n",
    "    else:\n",
    "        dfc = df.copy(deep=True)\n",
    "                           \n",
    "    X = dfc[save_fields] \n",
    "\n",
    "    if signal:\n",
    "        X['isSignal']  = np.ones(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = [signal]*X['weight'].shape[0]\n",
    "    else: \n",
    "        X['isSignal'] = np.zeros(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = ['QCD']*X['weight'].shape[0]\n",
    "    del dfc\n",
    "    return X\n",
    "\n",
    "def get_sum_genweights(data_dir: Path, dataset: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the sum of genweights for a given dataset.\n",
    "    :param data_dir: The directory where the datasets are stored.\n",
    "    :param dataset: The name of the dataset to get the genweights for.\n",
    "    :return: The sum of genweights for the dataset.\n",
    "    \"\"\"\n",
    "    total_sumw = 0\n",
    "    try:\n",
    "        # Load the genweights from the pickle file\n",
    "        for pickle_file in list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")):\n",
    "            with Path(pickle_file).open(\"rb\") as file:\n",
    "                out_dict = pickle.load(file)\n",
    "            # The sum of weights is stored in the \"sumw\" key\n",
    "            # You can access it like this:\n",
    "            for key in out_dict:\n",
    "                sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "            total_sumw += sumw\n",
    "        print(pickle_file)\n",
    "    except:\n",
    "        print(\"shit: \", list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\"))[0])\n",
    "        warnings.warn(\n",
    "            f\"Error loading genweights for dataset: {dataset}. Skipping.\",\n",
    "            category=UserWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        total_sumw = 1\n",
    "\n",
    "    # print(f\"Total sum of weights for all pickles for {dataset}: {total_sumw}\")\n",
    "    return total_sumw\n",
    "\n",
    "def accumulator(proc, isSignal=False, shallow=False, path=None): #perform data accumulation for a particular process\n",
    "    if path is None:\n",
    "        data_dir = '/uscms/home/bweiss/nobackup/hbb/'\n",
    "        dirs = get_paths('2023', data_dir, proc)\n",
    "        # print(dirs)\n",
    "    else:\n",
    "        if os.path.isfile(path):\n",
    "            dirs = [path]\n",
    "        else: \n",
    "            dirs = os.listdir(path)\n",
    "    # dataset = None\n",
    "    all_data = None\n",
    "    # total = 0\n",
    "    # for d in dirs:\n",
    "    #     if shallow:\n",
    "    #         total += min(len(os.listdir(d)), shallow)\n",
    "    #     else:\n",
    "    #         total += len(os.listdir(d))\n",
    "    # print(total)\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)): #runs through subsets of a process\n",
    "        dataset = None\n",
    "        if os.path.isfile(d):\n",
    "            ds = [d]\n",
    "        else:\n",
    "            ds = os.listdir(d)\n",
    "        # print(ds)\n",
    "        for i, file in enumerate(ds): #runs through files in subset\n",
    "            if shallow and i>shallow: #use only 1 parquet file from each subset if shallow\n",
    "                print(file)\n",
    "                break\n",
    "            file_path = os.path.join(d,file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            cols = df.columns\n",
    "            excluded_cols = ['MET', #'FatJet0_pt', 'FatJet0_msd', 'FatJet0_pnetMass', 'FatJet0_pnetTXbb'\n",
    "                            ]\n",
    "            save_cols = [c for c in cols if (c not in excluded_cols) \n",
    "                         and ('Gen' not in c)\n",
    "                         \n",
    "                        ]\n",
    "            # # save_cols = [col for col in multiindex_columns if isinstance(col, int) and col_string in col[0]]+['weight']\n",
    "            # save_cols = [col for col in multiindex_columns if ( (col_string in col[0]) #save all ak8fatjet columns and weights\n",
    "                                                            # and ('ass' not in col[0]) \n",
    "                                                            # and ('soft' not in col[0])\n",
    "                                                            #   )] \n",
    "            # save_cols = save_cols + [('weight', 0)] + [('weight_noxsec', 0)]\n",
    "            # if i == 0:\n",
    "            #      print('save_cols: ', save_cols)\n",
    "            \n",
    "            thisdf = process_single(df, cuts=True,\n",
    "                               save_fields = save_cols,\n",
    "                               signal = isSignal,\n",
    "                                  ) #apply cuts save select columns, add isSignal column\n",
    "            if dataset is None:\n",
    "                dataset = thisdf\n",
    "            else:\n",
    "                dataset = pd.concat([dataset, thisdf], axis = 0, ignore_index=True)\n",
    "            del thisdf\n",
    "        #reweight events but sum of weights in a MC dataset\n",
    "        this_dataset = Path(d).parent.parent.name\n",
    "        print(this_dataset)\n",
    "        sumW = get_sum_genweights(Path('/uscms/home/bweiss/nobackup/hbb/2023'), this_dataset)\n",
    "        # sumW = np.sum(dataset['weight'].values)\n",
    "        dataset['weight_final'] = abs(dataset['weight'])/sumW\n",
    "        print(f'sum of all weights in {d} is {sumW}')\n",
    "        dataset['MC_name'] = this_dataset\n",
    "        if all_data is None:\n",
    "            all_data = dataset\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, dataset], axis = 0, ignore_index=True)\n",
    "        # del dataset\n",
    "            \n",
    "    # print('save_cols: ', save_cols)\n",
    "    return all_data\n",
    "\n",
    "def df2Dmatrix(X):\n",
    "    #convert final df to dmatrix for xgb\n",
    "    dmatrix = xgb.DMatrix(X, label= X['isSignal'], missing = -9999, weight = X['weight_noxsec'])\n",
    "    return dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb36cd6-c94e-4b4a-ac97-ebc0650fc759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_hist(year, process, field, cut = True, full=True):\n",
    "    \n",
    "    data_dir = '/uscms/home/bweiss/nobackup/hbb/' #folder containing all samples\n",
    "    #collect all field data for the process\n",
    "    dirs = get_paths(year, data_dir, process)\n",
    "    # print(dirs)\n",
    "    var = None\n",
    "\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)+' '+str(field)):\n",
    "        # print(d)\n",
    "        for i, file in enumerate(os.listdir(d)):\n",
    "            path = os.path.join(d,file)\n",
    "            # print(path)\n",
    "            # print(path)\n",
    "            if var is None:\n",
    "                var = get_proc_field(path, field, cut=cut)\n",
    "                # print(var)\n",
    "            else:\n",
    "                var = np.append(var, get_proc_field(path, field, cut=cut), axis=0)\n",
    "                # print(var.shape, type(var))\n",
    "            # if not full:\n",
    "            #     print('stored var from only 1 parquet')\n",
    "            #     break\n",
    "    # var = var.flatten()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb7e48-0c4c-4c6e-8653-336b9ff14c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/uscms/home/bweiss/nobackup/hbb/2023/VBFHto2B_M-125_dipoleRecoilOn/parquet/signal-all/part0.parquet'\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "print(df.columns)\n",
    "print(df['weight'].head())\n",
    "\n",
    "# data = accumulator('VH', isSignal='test', shallow=100, path = path)\n",
    "\n",
    "# print(data.columns)\n",
    "# print('nJet: ', data['nFatJet'].head())\n",
    "# print(type(data['FatJet1_pt']), data['FatJet1_pt'].head())\n",
    "\n",
    "# for c in proc_data.columns:\n",
    "#     print(c, type(data[c]), type(data[c][10]))\n",
    "\n",
    "# proc_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa282433-c191-4935-9bdf-a94e8e5c7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_paths('2023', '/uscms/home/bweiss/nobackup/hbb/', 'VH')[0]\n",
    "print(d)\n",
    "d = Path(d)\n",
    "dataset = d.parent.parent.name\n",
    "print(dataset)\n",
    "\n",
    "data_dir = '/uscms/home/bweiss/nobackup/hbb/2023'\n",
    "\n",
    "sumW = get_sum_genweights(Path(data_dir), dataset)\n",
    "\n",
    "# example_pkl = '/uscms/home/bweiss/nobackup/hbb/2023/WminusH_Hto2B_Wto2Q_M-125/pickles/out_9.pkl'\n",
    "\n",
    "# with Path(example_pkl).open(\"rb\") as file:\n",
    "#     out_dict = pickle.load(file)\n",
    "# # The sum of weights is stored in the \"sumw\" key\n",
    "# # You can access it like this:\n",
    "# for key in out_dict:\n",
    "#     sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "\n",
    "print(sumW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813c623-eecf-4c56-9bbf-4afad6aba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Accumulate data, prepare it, save to mega DF '''\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning) \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# shallow = True #take only one parquet from each process/proc subset\n",
    "\n",
    "samples = ['VBF', 'VH', 'ggF', 'QCD'] #processes to aquire\n",
    "isSignal = ['VBF', 'VH', 'ggF', False]\n",
    "\n",
    "# samples = ['QCD']\n",
    "# isSignal = ['QCD']\n",
    "\n",
    "X = None\n",
    "\n",
    "for j, s in enumerate(samples):\n",
    "    # print(s)\n",
    "    proc_data = accumulator(s, isSignal=isSignal[j], shallow=False)\n",
    "    # print(proc_data.columns)\n",
    "    if X is None:\n",
    "        X = proc_data\n",
    "        # print(X.columns)\n",
    "    else:\n",
    "        X = pd.concat([X, proc_data], axis = 0, ignore_index=True)\n",
    "    # print(X['isSignal'].shape)\n",
    "    del proc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6d06f-1a2e-482f-ac88-d9e49d37e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[['isSignal', 'category', 'MC_name', 'weight_final']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6eb0c-cb4a-46ab-a35b-da8cc933c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.columns)\n",
    "# print(X[['weight', 'weight_final', 'isSignal']])\n",
    "MC_names = pmap['QCD']\n",
    "print(MC_names)\n",
    "\n",
    "\n",
    "cs = ['FatJet0_pt', 'FatJet1_pt', 'Jet0_pt', 'Jet1_pt']\n",
    "\n",
    "# del hist_stacks\n",
    "hist_stacks = None\n",
    "weights_stacks = None\n",
    "# hist_stacks.\n",
    "for i, MC_name in enumerate(MC_names):\n",
    "    mask = X['MC_name'] == MC_name\n",
    "    var = X['FatJet0_pt'][mask].reset_index(drop=True).values\n",
    "    ws = X['weight_final'][mask].reset_index(drop=True).values\n",
    "    # print(sum(mask))\n",
    "    # hist_stacks = pd.concat([ hist_stacks, X[cols][mask] ], axis = 0, ignore_index=True)\n",
    "    if hist_stacks is None:\n",
    "        hist_stacks = [var]\n",
    "        # print(hist_stacks)\n",
    "        weights_stacks = [ws] \n",
    "        # hist_stacks.columns = ['FatJet0_pt']\n",
    "        # print(hist_stacks)\n",
    "        # print(X['FatJet0_pt'][mask].values)\n",
    "    else:\n",
    "        hist_stacks += [var]\n",
    "        weights_stacks += [ws]\n",
    "        \n",
    "        print(len(hist_stacks))\n",
    "    print(i)\n",
    "    \n",
    "    # ax[0][0].hist(X['FatJet0_pt'][mask], stacked = True, histtype='barstacked', #density=True, \n",
    "    #         bins=bins, linewidth=2, weights = X['weight_final'][mask], label = 'w/sum_w')\n",
    "    # # # ax[0][0].hist(X['FatJet0_pt'][mask], histtype='step', density=True, \n",
    "    # # #         bins=bins, linewidth=2, weights = X['weight'][mask], label = 'w_noNorm')\n",
    "    # ax[0][0].set(yscale = 'log', xlabel = 'FatJet0_pt')\n",
    "    \n",
    "    # ax[0][1].hist(X['FatJet1_pt'][mask], stacked = True, histtype='barstacked', #density=True, \n",
    "    #         bins=bins, linewidth=2, weights = X['weight_final'][mask], label = 'w/sum_w')\n",
    "    # # ax[0][1].hist(X['FatJet1_pt'][mask], histtype='step', density=True, \n",
    "    # #         bins=bins, linewidth=2, weights = X['weight'][mask], label = 'w_noNorm')\n",
    "    # ax[0][1].set(yscale = 'log', xlabel = 'FatJet1_pt')\n",
    "    \n",
    "    # ax[1][0].hist(X['Jet0_pt'][mask], stacked = True, histtype='barstacked', #density=True, \n",
    "    #         bins=bins, linewidth=2, weights = X['weight_final'][mask], label = 'w/sum_w')\n",
    "    # # ax[1][0].hist(X['Jet0_pt'][mask], histtype='step', density=True, \n",
    "    # #         bins=bins, linewidth=2, weights = X['weight'][mask], label = 'w_noNorm')\n",
    "    # ax[1][0].set(yscale = 'log', xlabel = 'Jet0_pt')\n",
    "    \n",
    "    # ax[1][1].hist(X['Jet1_pt'][mask], stacked = True, histtype='barstacked', #density=True, \n",
    "    #         bins=bins, linewidth=2, weights = X['weight_final'][mask], label = 'w/sum_w')\n",
    "    # # ax[1][1].hist(X['Jet1_pt'][mask], histtype='step', density=True, \n",
    "    # #         bins=bins, linewidth=2, weights = X['weight'][mask], label = 'w_noNorm')\n",
    "    # ax[1][1].set(yscale = 'log', xlabel = 'Jet1_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8aea8-cf5f-4b93-b4e3-24a9456d9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hist_stacks))\n",
    "fig, ax = plt.subplots(1, 2, figsize = (9,5))\n",
    "bins = np.linspace(400,1500, 100)\n",
    "\n",
    "ax[0].hist(hist_stacks, histtype='barstacked', #density=True, \n",
    "        bins=bins, linewidth=2, weights = weights_stacks, label = 'w/sum_w')\n",
    "# ax[1].hist(, stacked = True, #histtype='barstacked', #density=True, \n",
    "#         bins=bins, linewidth=2, #weights = weights_stacks, label = 'w/sum_w'\n",
    "#           )\n",
    "# # ax[0][0].hist(X['FatJet0_pt'][mask], histtype='step', density=True, \n",
    "# #         bins=bins, linewidth=2, weights = X['weight'][mask], label = 'w_noNorm')\n",
    "ax[0].set(yscale = 'log', xlabel = 'FatJet0_pt')\n",
    "ax[1].set(yscale = 'log', xlabel = 'weights_final')\n",
    "\n",
    "samples = ['VBF', 'VH', 'ggF', 'QCD']\n",
    "\n",
    "for s in samples:\n",
    "    mask = X['category'] == s\n",
    "    Ws = X['weight_final'][mask]\n",
    "    print(f'{s} has yield:{sum(Ws)}')\n",
    "    ax[1].hist(Ws, stacked = False, histtype='step', #density=True, \n",
    "         linewidth=2, #weights = weights_stacks, label = 'w/sum_w'\n",
    "\n",
    "ax[0].legend(MC_names, loc = 'upper right')\n",
    "\n",
    "plt.suptitle('QCD Jet Pt reweighting')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('QCD_Jet_Pt_reweighting3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6a9c1-a1ec-4495-a723-c5f38bf5214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak4eta_cols = ['Jet0_eta', 'Jet1_eta', 'Jet2_eta','Jet3_eta']\n",
    "jj_pairs = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n",
    "\n",
    "ak4_etas = X[ak4eta_cols]\n",
    "\n",
    "for jj in jj_pairs:\n",
    "    jet1_eta, jet2_eta = ak4_etas[ak4eta_cols(jj[0])], ak4_etas[ak4eta_cols(jj[1])]\n",
    "    dEta_jj_all = ak4_etas[f'jj_{jj}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adb7f4-9235-484b-ab7d-4342302fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual omition of negative weights\n",
    "# X= proc_data\n",
    "print(X.columns)\n",
    "\n",
    "# X['weight']=abs(X['weight'])\n",
    "# X['isSignal'] = X['isSignal'].astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42, shuffle=True, #stratify = X['isSignal']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325039c-c69e-475b-b81c-41b07eb0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BDT model\n",
    "# import xgboost as xgb\n",
    "\n",
    "# see https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\n",
    "# for detailed explanations of parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6adc7-3eb7-468c-9362-16395fc7501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_cols = ['isSignal', 'weight','weight_final', \n",
    "             'category', 'FatJet0_pt', 'FatJet0_msd', \n",
    "             'FatJet0_pnetMass', 'FatJet0_pnetTXbb', 'MC_name'\n",
    "            ]\n",
    "\n",
    "Y_train = X_train['isSignal']\n",
    "Y_test = X_test['isSignal']\n",
    "W_train = X_train['weight_final']\n",
    "W_test = X_test['weight_final']\n",
    "pos_weight = sum(W_train[X_train['isSignal'] == 0])/sum(W_train[X_train['isSignal'] == 1])\n",
    "print(pos_weight)\n",
    "# print('Y_train: ',Y_train.head(), sum(Y_train))\n",
    "# print('Y_test: ', sum(Y_test))\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # number of boosting rounds (i.e. number of decision trees)\n",
    "    max_depth=8,  # max depth of each decision tree\n",
    "    learning_rate=0.1,\n",
    "    early_stopping_rounds=20,  #Remove this # how many rounds to wait to see if the loss is going down\n",
    "    missing = np.nan,\n",
    "    scale_pos_weight = pos_weight,\n",
    "    # eval_metric='logloss',\n",
    "    # objective = 'binary:logistic'\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# print('X_train: ', X_train.drop(omit_cols, axis = 1).columns)\n",
    "#\n",
    "# print(X_train['Jet3_pt'], type(X_train['Jet3_pt'][3000]))\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# X_train['isSignal'] = le.fit_transform(X_train['isSignal'])\n",
    "# X_test['isSignal'] = le.fit_transform(X_test['isSignal'])\n",
    "\n",
    "\n",
    "trained_model = model.fit(\n",
    "    X_train.drop(omit_cols, axis=1), #data should not include label column OR weights\n",
    "    Y_train, #labels\n",
    "    sample_weight=W_train,\n",
    "    # Y_train_val,\n",
    "    # xgboost uses the last set for early stopping\n",
    "    # https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping\n",
    "    eval_set=[(X_train.drop(omit_cols, axis=1), Y_train), \n",
    "              (X_test.drop(omit_cols, axis=1), Y_test)],  # sets for which to save the loss\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59e8e8-ea47-4a2b-bc95-c6532e6f07d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evals_result = trained_model.evals_result()\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "for i, label in enumerate([\"Train\", \"Test\"]):\n",
    "    plt.plot(evals_result[f\"validation_{i}\"][\"logloss\"], label=label, linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot ROC\n",
    "Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "\n",
    "# Y_predict = Y_predict[:, 1].squeeze()\n",
    "# print(Y_predict)\n",
    "# Y_predict = le.inverse_transform(Y_predict)\n",
    "# print(Y_predict)\n",
    "# X_test['isSignal'] = le.inverse_transform(X_test['isSignal'])\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "samples = [False, 'VBF', 'ggF',  'VH']\n",
    "for i, s in enumerate(samples[1:]):\n",
    "    category_mask = ((X_test['category'] == s) | (X_test['category'] == 'QCD'))\n",
    "    fpr, tpr, thresholds = roc_curve(X_test[category_mask]['isSignal'].astype(int), \n",
    "                                     Y_predict[category_mask,1], \n",
    "                                     sample_weight = X_test[category_mask]['weight_final'],\n",
    "                                     pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"{s} auc = %.3f\" % (roc_auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"random chance\")\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel(\"false positive rate\")\n",
    "ax.set_ylabel(\"true positive rate\")\n",
    "ax.set_title(\"receiver operating curve\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343f573-b205-4cff-b923-49d0f5d99b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (5,4))\n",
    "samples = ['QCD', 'VBF', 'ggF',  'VH', ]\n",
    "colors = ['black', [0.6,1,0.6], [0,0.8,0], [0,0.3,0]]\n",
    "print(Y_predict[:,1].shape)\n",
    "print(X_test['weight'].shape)\n",
    "\n",
    "bins = np.linspace(0,1,10)\n",
    "signals = pd.DataFrame()\n",
    "# weights = pd.DataFrame()\n",
    "for i, s in enumerate(samples[1:]):\n",
    "    category_mask = X_test['category'] == s\n",
    "    # w = pd.DataFrame(X_test['weight_final'][category_mask], columns=[s])\n",
    "    data = pd.DataFrame(Y_predict[category_mask, 1], columns=[s])\n",
    "    signals = pd.concat([signals, data], axis=1)\n",
    "    # weigths = pd.concat([weights, w], axis=1)\n",
    "    # if s is not 'QCD':\n",
    "    #     # ax.hist(Y_predict[category_mask, 1], color = colors[i], label = samples[i], stacked=True, linewidth=3, bins=bins #weights = w\n",
    "    #        )\n",
    "    # else:\n",
    "    #     ax.hist(Y_predict[category_mask, 1], histtype='step', color = colors[i], label = samples[i], stacked=False, linewidth=3, bins=bins #weights = w\n",
    "    #        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "QCD_mask =  X_test['category'] == 'QCD'\n",
    "ax.hist(Y_predict[QCD_mask, 1], histtype='step', color = colors[0], label = samples[0], stacked=False, linewidth=3, bins=bins #weights = w\n",
    "           )\n",
    "ax.hist(signals, #color = colors[1:],\n",
    "        label =signals.columns, stacked=True, linewidth=3, bins=bins, #weights = w\n",
    "           )\n",
    "    \n",
    "ax.set_yscale('log')\n",
    "ax.legend(samples)\n",
    "ax.set_ylabel('Events')\n",
    "ax.set_xlabel('BDT score')\n",
    "ax.set_title('S/B classifer scores by channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae977c-be53-402f-b089-8e2dbc4f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a6c3d-00c2-429f-a9cf-86d4edc9d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained_model.feature_importances_)\n",
    "plt.figure(figsize=(9,18))\n",
    "# plot\n",
    "# c = X_train.columns\n",
    "# fields = np.unique(np.array([c[0] for c in X_train.columns]))\n",
    "\n",
    "# fields=np.array([c[0] for c in X_train.columns])\n",
    "features = trained_model.get_booster().feature_names\n",
    "importance = trained_model.feature_importances_\n",
    "print(importance)\n",
    "# y = range(len(importance))\n",
    "\n",
    "\n",
    "fi = pd.DataFrame({'features': features, 'importance': importance})\n",
    "fi.sort_values(by = 'importance', ascending=False).reset_index(drop=True)\n",
    "# print(fi['importance'])\n",
    "# print(fi)\n",
    "\n",
    "y=range(len(fi))\n",
    "plt.barh(y, fi['importance'])\n",
    "# plt.invert_yaxis()\n",
    "# plt.bar(fields, trained_model.feature_importances_)\n",
    "plt.yticks(y, labels=fi['features'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948a1d8-a7d8-46e5-9fab-dc0dc017e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train['Jet3_pt'])\n",
    "print(X_train['Jet3_pt'].loc[7201776])\n",
    "print(type(X_train['Jet3_pt'].loc[7201776]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655bf57-6576-4bac-a7dc-86874fa19416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

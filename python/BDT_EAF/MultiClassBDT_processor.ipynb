{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4002-114f-485e-a39f-d614e095b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import awkward as ak\n",
    "# import dask\n",
    "import json\n",
    "# from coffea import processor\n",
    "# from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import hist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.CMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb1de3-40f1-4d21-bc80-424ff333e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('samples_noQCD2000.json', 'r') as file:\n",
    "with open('samples.json', 'r') as file:\n",
    "    pmap = json.load(file)\n",
    "\n",
    "print(pmap.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d79fe-d2ad-49e0-9976-d7265672c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''processing functions'''\n",
    "'''Cut definitions'''\n",
    "def minjetkin(df):\n",
    "    # fatjets = df['ak8FatJetmsoftdrop', 'ak8FatJetPt', 'ak8FatJetEta']\n",
    "    # print(df['ak8FatJetmsoftdrop'].shape)\n",
    "    fatjet_msd = df['FatJet0_msd'].values\n",
    "    fatjet_pt = df['FatJet0_pt'].values\n",
    "    fatjet_eta = df['FatJet0_eta'].values\n",
    "        # fatjets['msdcorr'] = fatjets.msoftdrop\n",
    "        # fatjets['qcdrho'] = 2 * np.log(fatjets.msdcorr / fatjets.pt)\n",
    "    candidatejet = df[\n",
    "            (fatjet_pt > 200)\n",
    "            & (abs(fatjet_eta) < 2.5)\n",
    "            # & fatjets.isTight \n",
    "        ]\n",
    "\n",
    "    # candidatejet = candidatejet[:, :2]\n",
    "    # candidatejet = ak.firsts(candidatejet[ak.argmax(candidatejet.particleNet_XbbVsQCD, axis=1, keepdims=True)])\n",
    "\n",
    "    # bvl = candidatejet.particleNet_XbbVsQCD\n",
    "    minjetkin=np.array([\n",
    "            (candidatejet['FatJet0_pt'] >= 450)\n",
    "            & (candidatejet['FatJet0_pt']< 1200)\n",
    "            & (candidatejet['FatJet0_msd'] >= 40.)\n",
    "            & (candidatejet['FatJet0_msd'] < 201.)\n",
    "            & (abs(candidatejet['FatJet0_eta']) < 2.5)\n",
    "       ])\n",
    "    # minjetkin=np.sum(minjetkin, axis=1).astype('bool').transpose()\n",
    "    minjetkin = minjetkin.astype('bool').transpose()\n",
    "\n",
    "    # print(minjetkin)\n",
    "    # print(minjetkin.shape)\n",
    "    # print(minjetkin)\n",
    "    \n",
    "    return df[minjetkin]\n",
    "    \n",
    "def get_paths(year, data_path, proc = 'QCD', deep=False):\n",
    "    #returns list of paths to parquet files\n",
    "    parquet_parents = [os.path.join(data_path, year, p, 'parquet','signal-all') for p in pmap[proc]]\n",
    "    \n",
    "    if deep:\n",
    "        file_list=None\n",
    "        for parent in parquet_parents:\n",
    "            if file_list is None:\n",
    "                file_list = [os.path.join(parent,file)for file in os.listdir(parent)]\n",
    "            else:\n",
    "                file_list = np.append(file_list, [os.path.join(parent,file)for file in os.listdir(parent)])\n",
    "    else:\n",
    "        file_list=parquet_parents\n",
    "    return file_list\n",
    "\n",
    "def mode2category(mode):\n",
    "    cats = np.array(['ggF', 'VBF', 'VH'])\n",
    "    if mode not in cats:\n",
    "        raise ValueError(f'Decay mode {mode} not in {cats}')\n",
    "    category = (cats==mode).astype(int)\n",
    "    return category\n",
    "\n",
    "print(mode2category('VBF'))\n",
    "        \n",
    "    \n",
    "    \n",
    "def process_single(df, \n",
    "                   cuts=False,\n",
    "                   save_fields = ['weight','FatJet0_pt'],\n",
    "                   signal = False,\n",
    "                   category = 'QCD', #category order: ['ggF', 'VBF', 'VH']\n",
    "               ):    \n",
    "    if cuts: \n",
    "        dfc = minjetkin(df.copy(deep=True))\n",
    "        #add more cuts here\n",
    "    else:\n",
    "        dfc = df.copy(deep=True)\n",
    "                           \n",
    "    X = dfc[save_fields] \n",
    "\n",
    "    if signal:\n",
    "        X['isSignal']  = np.ones(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = [signal]*X['weight'].shape[0]\n",
    "        # X['y'] = mode2category(X['category'])*X['weight'].shape[0]\n",
    "    else: \n",
    "        X['isSignal'] = np.zeros(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = ['QCD']*X['weight'].shape[0]\n",
    "        # X['y'] = np.array([0,0,0]*X['weight'].shape[0])\n",
    "    del dfc\n",
    "    return X\n",
    "\n",
    "def get_sum_genweights(data_dir: Path, dataset: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the sum of genweights for a given dataset.\n",
    "    :param data_dir: The directory where the datasets are stored.\n",
    "    :param dataset: The name of the dataset to get the genweights for.\n",
    "    :return: The sum of genweights for the dataset.\n",
    "    \"\"\"\n",
    "    total_sumw = 0\n",
    "    try:\n",
    "        # Load the genweights from the pickle file\n",
    "        for pickle_file in list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")):\n",
    "            with Path(pickle_file).open(\"rb\") as file:\n",
    "                out_dict = pickle.load(file)\n",
    "            # The sum of weights is stored in the \"sumw\" key\n",
    "            # You can access it like this:\n",
    "            for key in out_dict:\n",
    "                sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "            total_sumw += sumw\n",
    "        print(pickle_file)\n",
    "    except:\n",
    "        print(\"shit: \", list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\"))[0])\n",
    "        warnings.warn(\n",
    "            f\"Error loading genweights for dataset: {dataset}. Skipping.\",\n",
    "            category=UserWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        total_sumw = 1\n",
    "\n",
    "    # print(f\"Total sum of weights for all pickles for {dataset}: {total_sumw}\")\n",
    "    return total_sumw\n",
    "\n",
    "def accumulator(proc, isSignal=False, shallow=False, path=None): #perform data accumulation for a particular process\n",
    "    if path is None:\n",
    "        data_dir = '/uscms/home/bweiss/nobackup/hbb/'\n",
    "        dirs = get_paths('2023', data_dir, proc)\n",
    "        # print(dirs)\n",
    "    else:\n",
    "        if os.path.isfile(path):\n",
    "            dirs = [path]\n",
    "        else: \n",
    "            dirs = os.listdir(path)\n",
    "    # dataset = None\n",
    "    all_data = None\n",
    "    # total = 0\n",
    "    # for d in dirs:\n",
    "    #     if shallow:\n",
    "    #         total += min(len(os.listdir(d)), shallow)\n",
    "    #     else:\n",
    "    #         total += len(os.listdir(d))\n",
    "    # print(total)\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)): #runs through subsets of a process\n",
    "        dataset = None\n",
    "        if os.path.isfile(d):\n",
    "            ds = [d]\n",
    "        else:\n",
    "            ds = os.listdir(d)\n",
    "        # print(ds)\n",
    "        for i, file in enumerate(ds): #runs through files in subset\n",
    "            if shallow and i>shallow: #use only 1 parquet file from each subset if shallow\n",
    "                print(file)\n",
    "                break\n",
    "            file_path = os.path.join(d,file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            cols = df.columns\n",
    "            excluded_cols = ['MET', #'FatJet0_pt', 'FatJet0_msd', 'FatJet0_pnetMass', 'FatJet0_pnetTXbb'\n",
    "                            ]\n",
    "            save_cols = [c for c in cols if (c not in excluded_cols) \n",
    "                         and ('Gen' not in c)\n",
    "                         \n",
    "                        ]\n",
    "            # # save_cols = [col for col in multiindex_columns if isinstance(col, int) and col_string in col[0]]+['weight']\n",
    "            # save_cols = [col for col in multiindex_columns if ( (col_string in col[0]) #save all ak8fatjet columns and weights\n",
    "                                                            # and ('ass' not in col[0]) \n",
    "                                                            # and ('soft' not in col[0])\n",
    "                                                            #   )] \n",
    "            # save_cols = save_cols + [('weight', 0)] + [('weight_noxsec', 0)]\n",
    "            # if i == 0:\n",
    "            #      print('save_cols: ', save_cols)\n",
    "            \n",
    "            thisdf = process_single(df, cuts=True,\n",
    "                               save_fields = save_cols,\n",
    "                               signal = isSignal,\n",
    "                                  ) #apply cuts save select columns, add isSignal column\n",
    "            if dataset is None:\n",
    "                dataset = thisdf\n",
    "            else:\n",
    "                dataset = pd.concat([dataset, thisdf], axis = 0, ignore_index=True)\n",
    "            del thisdf\n",
    "        #reweight events but sum of weights in a MC dataset\n",
    "        this_dataset = Path(d).parent.parent.name\n",
    "        print(this_dataset)\n",
    "        sumW = get_sum_genweights(Path('/uscms/home/bweiss/nobackup/hbb/2023'), this_dataset)\n",
    "        dataset['sumW'] = np.ones_like(dataset['weight'])*sumW\n",
    "        # sumW = np.sum(dataset['weight'].values)\n",
    "        dataset['weight_final'] = abs(dataset['weight'])/sumW\n",
    "        print(f'sum of all weights in {d} is {sumW}')\n",
    "        dataset['MC_name'] = this_dataset\n",
    "        if all_data is None:\n",
    "            all_data = dataset\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, dataset], axis = 0, ignore_index=True)\n",
    "        # del dataset\n",
    "            \n",
    "    # print('save_cols: ', save_cols)\n",
    "    return all_data\n",
    "\n",
    "def df2Dmatrix(X):\n",
    "    #convert final df to dmatrix for xgb\n",
    "    dmatrix = xgb.DMatrix(X, label= X['isSignal'], missing = -9999, weight = X['weight_noxsec'])\n",
    "    return dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb7e48-0c4c-4c6e-8653-336b9ff14c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/uscms/home/bweiss/nobackup/hbb/2023/VBFHto2B_M-125_dipoleRecoilOn/parquet/signal-all/part0.parquet'\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "print(df.columns)\n",
    "print(df['weight'].head())\n",
    "\n",
    "# data = accumulator('VH', isSignal='test', shallow=100, path = path)\n",
    "\n",
    "# print(data.columns)\n",
    "# print('nJet: ', data['nFatJet'].head())\n",
    "# print(type(data['FatJet1_pt']), data['FatJet1_pt'].head())\n",
    "\n",
    "# for c in proc_data.columns:\n",
    "#     print(c, type(data[c]), type(data[c][10]))\n",
    "\n",
    "# proc_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824b719-3428-42dd-8b5b-5284a156a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Accumulate data, prepare it, save to mega DF '''\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning) \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# shallow = True #take only one parquet from each process/proc subset\n",
    "\n",
    "samples = ['VBF', 'VH', 'ggF', 'ttH'#'QCD'\n",
    "          ] #processes to aquire\n",
    "isSignal = ['VBF', 'VH', 'ggF', 'ttH'#False\n",
    "           ]\n",
    "\n",
    "# samples = ['QCD']\n",
    "# isSignal = ['QCD']\n",
    "\n",
    "X = None\n",
    "\n",
    "for j, s in enumerate(samples):\n",
    "    # print(s)\n",
    "    proc_data = accumulator(s, isSignal=isSignal[j], shallow=False)\n",
    "    # print(proc_data.columns)\n",
    "    if X is None:\n",
    "        X = proc_data\n",
    "        # print(X.columns)\n",
    "    else:\n",
    "        X = pd.concat([X, proc_data], axis = 0, ignore_index=True)\n",
    "    # print(X['isSignal'].shape)\n",
    "    del proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813c623-eecf-4c56-9bbf-4afad6aba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(X['category'])\n",
    "X['y'] = le.transform(X['category'])\n",
    "print(X[['y', 'category']].sample(frac=1, random_state=11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6a9c1-a1ec-4495-a723-c5f38bf5214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.serif\": [\"Computer Modern Roman\"]\n",
    "# })\n",
    "plt.rcdefaults()\n",
    "\n",
    "ak4eta_cols = ['Jet0_eta', 'Jet1_eta', 'Jet2_eta','Jet3_eta', 'category']\n",
    "jj_pairs = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n",
    "\n",
    "ak4_etas = X[ak4eta_cols]\n",
    "dEta_jj_all = pd.DataFrame()\n",
    "\n",
    "#f'jj_{jj}'\n",
    "for jj in jj_pairs:\n",
    "    jet1_eta, jet2_eta = ak4_etas[ak4eta_cols[ jj[0] ]], ak4_etas[ak4eta_cols[ jj[1] ]]\n",
    "        \n",
    "    dEta_jj = pd.DataFrame()\n",
    "    dEta_jj[f'jj_{jj}'] = abs(jet2_eta-jet1_eta)\n",
    "    # dEta_jj['dEta'] = abs(jet2_eta-jet1_eta)\n",
    "    \n",
    "    dEta_jj_all = pd.concat([dEta_jj_all, dEta_jj], axis = 1)\n",
    "    print(dEta_jj_all.columns)\n",
    "\n",
    "dEta_jj_max = pd.DataFrame(np.nanmax(dEta_jj_all.values, axis = 1), columns = ['max'])\n",
    "\n",
    "dEta_jj_all = pd.concat([dEta_jj_all, dEta_jj_max], axis = 1)\n",
    "dEta_jj_all = pd.concat([dEta_jj_all, X['category']], axis = 1)\n",
    "\n",
    "# print(dEta_jj_all)\n",
    "\n",
    "stacks = pd.DataFrame()\n",
    "samples = ['QCD', 'VBF', 'ggF',  'VH']\n",
    "\n",
    "for s in samples:\n",
    "    cat_mask = dEta_jj_all['category'] == s\n",
    "    subset = pd.DataFrame(dEta_jj_all['max'][cat_mask].values, columns = [s])\n",
    "    stacks = pd.concat([stacks, subset], axis = 1)\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5) )\n",
    "ax.hist(stacks, stacked = False, label = stacks.columns, histtype = 'step', lw = 2)\n",
    "ax.set(yscale = 'log', xlabel = 'max(dEta_jj)')\n",
    "# ax.legend(samples)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# plt.hist(dEta_jj_all['max'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adb7f4-9235-484b-ab7d-4342302fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual omition of negative weights\n",
    "# X= proc_data\n",
    "print(X.columns)\n",
    "\n",
    "# X['weight']=abs(X['weight'])\n",
    "# X['isSignal'] = X['isSignal'].astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42, shuffle=True, #stratify = X['isSignal']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325039c-c69e-475b-b81c-41b07eb0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BDT model\n",
    "# import xgboost as xgb\n",
    "\n",
    "# see https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\n",
    "# for detailed explanations of parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b56de5-8503-487c-ad2e-51fdeb026c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train['category']!='ttH')\n",
    "print(X_train['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6adc7-3eb7-468c-9362-16395fc7501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_cols = ['isSignal', 'weight','weight_final', \n",
    "             'category', 'FatJet0_pt', 'FatJet0_msd', 'FatJet0_msdmatched',\n",
    "             'MC_name', 'y', 'sumW', 'genWeight',\n",
    "             'FatJet0_pnetMass', 'FatJet0_pnetTXbb', 'FatJet0_pnetTXgg',\n",
    "             'FatJet0_pnetTXcc', 'FatJet0_pnetTXqq', 'FatJet0_pnetXbbXcc', 'FatJet0_pnetTQCD',\n",
    "             # 'FatJet1_pnetMass', 'FatJet1_pnetTXbb', 'FatJet1_pnetTXcc',\n",
    "             # 'FatJet1_pnetTXqq', 'FatJet1_pnetTXgg',\n",
    "             # 'Jet0_btagPNetB', 'Jet0_btagPNetCvB', 'Jet0_btagPNetCvL', 'Jet0_btagPNetQvG',\n",
    "             # 'Jet1_btagPNetB', 'Jet1_btagPNetCvB', 'Jet1_btagPNetCvL', 'Jet1_btagPNetQvG',\n",
    "             # 'Jet2_btagPNetB', 'Jet2_btagPNetCvB', 'Jet2_btagPNetCvL', 'Jet2_btagPNetQvG',\n",
    "             # 'Jet3_btagPNetB', 'Jet4_btagPNetCvB', 'Jet4_btagPNetCvL', 'Jet4_btagPNetQvG',\n",
    "            ]\n",
    "train_not_ttH = X_train['category']!='ttH'\n",
    "test_not_ttH = X_test['category']!='ttH'\n",
    "print(len(X_train))\n",
    "X_train = X_train[train_not_ttH]\n",
    "print(len(X_train))\n",
    "\n",
    "Y_train = X_train['y']\n",
    "Y_test = X_test['y']\n",
    "W_train = X_train['weight_final']\n",
    "W_test = X_test['weight_final']\n",
    "pos_weight = sum(W_train[X_train['isSignal'] == 0])/sum(W_train[X_train['isSignal'] == 1])\n",
    "print(pos_weight)\n",
    "# print('Y_train: ',Y_train.head(), sum(Y_train))\n",
    "# print('Y_test: ', sum(Y_test))\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # number of boosting rounds (i.e. number of decision trees)\n",
    "    max_depth=8,  # max depth of each decision tree\n",
    "    learning_rate=0.5,\n",
    "    early_stopping_rounds=20,  #Remove this # how many rounds to wait to see if the loss is going down\n",
    "    missing = np.nan,\n",
    "    # scale_pos_weight = pos_weight,\n",
    "    eval_metric='merror',\n",
    "    objective = 'multi:softmax',\n",
    "    num_classes = 3\n",
    "    \n",
    ")\n",
    "\n",
    "trained_model = model.fit(\n",
    "    X_train.drop(omit_cols, axis=1), #data should not include label column OR weights\n",
    "    Y_train, #labels\n",
    "    sample_weight=W_train,\n",
    "    # Y_train_val,\n",
    "    # xgboost uses the last set for early stopping\n",
    "    # https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping\n",
    "    eval_set=[(X_train.drop(omit_cols, axis=1), Y_train), \n",
    "              (X_test[test_not_ttH].drop(omit_cols, axis=1), Y_test[test_not_ttH])],  # sets for which to save the loss\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59e8e8-ea47-4a2b-bc95-c6532e6f07d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "print('Pred. class:', np.argmax(Y_predict, axis = 1))\n",
    "print('True class:', Y_test.values)\n",
    "\n",
    "evals_result = trained_model.evals_result()\n",
    "# print(evals_result['validation_0'].keys())\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "for i, label in enumerate([\"Train\", \"Test\"]):\n",
    "    plt.plot(evals_result[f\"validation_{i}\"][\"merror\"], label=label, linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot ROC\n",
    "# Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "\n",
    "# # Y_predict = Y_predict[:, 1].squeeze()\n",
    "# # print(Y_predict)\n",
    "# # Y_predict = le.inverse_transform(Y_predict)\n",
    "# # print(Y_predict)\n",
    "# # X_test['isSignal'] = le.inverse_transform(X_test['isSignal'])\n",
    "\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "# samples = [False, 'VBF', 'ggF',  'VH']\n",
    "# for i, s in enumerate(samples[1:]):\n",
    "#     category_mask = ((X_test['category'] == s) | (X_test['category'] == 'QCD'))\n",
    "#     fpr, tpr, thresholds = roc_curve(X_test[category_mask]['isSignal'].astype(int), \n",
    "#                                      Y_predict[category_mask,1], \n",
    "#                                      sample_weight = X_test[category_mask]['weight_final'],\n",
    "#                                      pos_label=1)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#     ax.plot(fpr, tpr, lw=2, label=f\"{s} auc = %.3f\" % (roc_auc))\n",
    "# ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"random chance\")\n",
    "# ax.set_xlim([0, 1.0])\n",
    "# ax.set_ylim([0, 1.0])\n",
    "# ax.set_xlabel(\"false positive rate\")\n",
    "# ax.set_ylabel(\"true positive rate\")\n",
    "# ax.set_title(\"receiver operating curve\")\n",
    "# ax.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986acb5b-a226-4c10-afd4-1ff5017c25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "main_dir = '/uscms/home/bweiss/nobackup/hbb/'\n",
    "categories = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "# proc = 'VH'\n",
    "CutBased_events = pd.DataFrame()\n",
    "\n",
    "for proc in categories:\n",
    "    paths = get_paths('2023', main_dir, proc = proc)\n",
    "    for samp in paths:\n",
    "        print(samp)\n",
    "        k = 0\n",
    "        for region in os.listdir(str(Path(samp).parent)):\n",
    "            if k==0:\n",
    "                print(f'Processing MC: {proc} categorized as {region}')\n",
    "            reg_in_cat = os.path.join(Path(samp).parent, region)\n",
    "            for p in os.listdir(reg_in_cat):\n",
    "                X = pd.read_parquet(os.path.join(reg_in_cat, p), columns = ['weight'])\n",
    "                X['true_cat'] = proc\n",
    "                X['cutBased_cat'] = region\n",
    "                CutBased_events = pd.concat([CutBased_events, X], axis = 0, ignore_index=True)\n",
    "            k+=1               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c569d1c-282f-4e8c-8ab1-be78d48dd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test['category'])\n",
    "for cat in categories:\n",
    "    cat_mask = X_test['category'] == cat\n",
    "    # if cat=='VBF':\n",
    "        # print(cat_mask)\n",
    "        # print(X_test[cat_mask]['sumW'].values)\n",
    "    sumW = np.nanmean(X_test[cat_mask]['sumW'].values)\n",
    "    print(sumW)\n",
    "    # print(cat, CutBased_events['true_cat'] == cat)\n",
    "    CutBased_events['weights_true'] = CutBased_events[CutBased_events['true_cat'] == cat]['weight'].values/sumW\n",
    "\n",
    "print(CutBased_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe17a6-bba9-45cc-b058-4bdbeb3dada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Purity plot'''\n",
    "\n",
    "def purity_plot(ax, X_test, category = '', use_weights = False, label =''): # produce a stacked histogram bar with the makeup of that category\n",
    "    # cat_mask = X_test['category'] == category\n",
    "    Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "    Y_predict = np.argmax(Y_predict, axis = 1)\n",
    "    cat_index = le.transform([category])\n",
    "    pred_mask = Y_predict == cat_index\n",
    "    X_pred = X_test[pred_mask]\n",
    "    use_weights = int(use_weights)\n",
    "    yields = np.zeros((4,2))\n",
    "    bottom = 0\n",
    "    for cat in range(4):\n",
    "        truecat = le.inverse_transform([cat])[0]\n",
    "        cat_mask = X_pred['category'] == truecat\n",
    "        yields[cat][:] = [np.sum(cat_mask), \n",
    "                          np.sum(X_pred['weight_final'][cat_mask].values, axis = 0)]\n",
    "    sumw = np.sum(yields[:,1])\n",
    "    yields[:,1] = yields[:,1]/ np.sum(yields[:,1])\n",
    "    for c in range(4):\n",
    "        ax.bar(category, yields[c, use_weights], bottom=bottom)\n",
    "        bottom += yields[c,use_weights]\n",
    "        if use_weights:\n",
    "            ax.set_ylim([0,1])\n",
    "        ax.set_title(f'n={round(np.sum(yields[:,0]))} \\n yield={round(sumw, 2)}', fontsize=16)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f'Predicted {category} purity: {yields[cat_index,1]})')\n",
    "    print(f'contains {yields[0][0]} VBF, {yields[1][0]} VH, {yields[2][0]} ggF')\n",
    "    print(f'with yields: {yields[0][1]} VBF, {yields[1][1]} VH, {yields[2][1]} ggF')\n",
    "\n",
    "# def cutBased_purity(ax, df, category = '', use_weights = False, label =''):\n",
    "#     cut_mask = CutBased_events['true_cat']\n",
    "\n",
    "    # return sumw\n",
    "    # bins = [0,1]\n",
    "    # _, _, patches = \n",
    "    # print(le.transform([category]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "fig, axes  = plt.subplots(1,3, figsize = (10,6))\n",
    "categories = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "for i in range(3):\n",
    "    a = purity_plot(axes[i], X_test, use_weights = True, category = categories[i])\n",
    "fig.legend(categories, ncol=4, loc = 'lower center', bbox_to_anchor=(0.5, -0.07))\n",
    "fig.suptitle('BDT purity', x=0.5, y=0.92)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244095c-b932-49bb-a000-381b796ed389",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get Cut based yields'''\n",
    "from pathlib import Path\n",
    "main_dir = '/uscms/home/bweiss/nobackup/hbb/'\n",
    "categories = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "# proc = 'VH'\n",
    "CutBased_events = pd.DataFrame()\n",
    "CBtoBDT_cats = {'signal-vh': 'VH',\n",
    "                'signal-ggf': 'ggF',\n",
    "                'signal-vbf': 'VBF',\n",
    "                'VH': 'signal-vh',\n",
    "                'ggF': 'signal-ggf',\n",
    "                'VBF': 'signal-vbf',}\n",
    "CB_cats = ['signal-vh','signal-ggf','signal-vbf']\n",
    "print(list(CBtoBDT_cats.keys()))\n",
    "for proc in categories:\n",
    "    paths = get_paths('2023', main_dir, proc = proc)\n",
    "    for samp in paths:\n",
    "        # print(samp)\n",
    "        MC_name = os.path.basename(Path(samp).parent.parent)\n",
    "        k = 0\n",
    "        for region in os.listdir(str(Path(samp).parent)):\n",
    "            if any(sig in region for sig in CB_cats):\n",
    "                if k==0:\n",
    "                    print(f'Processing MC: {MC_name} categorized as {region}')\n",
    "                reg_in_cat = os.path.join(Path(samp).parent, region)\n",
    "                for p in os.listdir(reg_in_cat):\n",
    "                    # print(p)\n",
    "                    X = pd.read_parquet(os.path.join(reg_in_cat, p), columns = ['weight'])\n",
    "                    X['true_cat'] = proc\n",
    "                    X['CB_cat'] = CBtoBDT_cats[region]\n",
    "                    # X['CB_cat'] = region\n",
    "                    sumW_file = './sumW.json'\n",
    "                    with open(sumW_file, 'r') as json_file:\n",
    "                        sumW_dict = json.load(json_file)\n",
    "                        sumW = sumW_dict[MC_name]\n",
    "                    X['weight_final'] = X['weight']/sumW\n",
    "                    CutBased_events = pd.concat([CutBased_events, X], axis = 0, ignore_index=True)\n",
    "                k+=1               \n",
    "\n",
    "print(CutBased_events)\n",
    "# print(os.listdir(str(Path(samp).parent)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96d0a2-f914-46c2-9a2c-5f7b3ee3cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot cut based purity'''\n",
    "def cutBased_purity(ax, X, category = '', use_weights = False, label =''): # produce a stacked histogram bar with the makeup of that category\n",
    "    # cat_mask = X_test['category'] == category\n",
    "    # Y_predict = X[cutBased_cat]\n",
    "    # Y_predict = np.argmax(Y_predict, axis = 1)\n",
    "    cat_index = le.transform([category])\n",
    "    pred_mask = X['CB_cat'] == category\n",
    "    X_pred = X[pred_mask]\n",
    "    use_weights = int(use_weights)\n",
    "    yields = np.zeros((4,2))\n",
    "    bottom = 0\n",
    "    for cat in range(4):\n",
    "        truecat = le.inverse_transform([cat])[0]\n",
    "        cat_mask = X_pred['true_cat'] == truecat\n",
    "        yields[cat][:] = [np.sum(cat_mask), \n",
    "                          np.sum(X_pred['weight_final'][cat_mask].values, axis = 0)]\n",
    "    sumw = np.sum(yields[:,1])\n",
    "    yields[:,1] = yields[:,1]/sumw\n",
    "    for c in range(4):\n",
    "        ax.bar(category, yields[c, use_weights], bottom=bottom)\n",
    "        bottom += yields[c,use_weights]\n",
    "        if use_weights:\n",
    "            ax.set_ylim([0,1])\n",
    "        ax.set_title(f'n={round(np.sum(yields[:,0]))} \\n yield={round(sumw, 2)}', fontsize=16)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f'Predicted {category} purity: {yields[cat_index,1]})')\n",
    "    print(f'contains {yields[0][0]} VBF, {yields[1][0]} VH, {yields[2][0]} ggF')\n",
    "    print(f'with yields: {yields[0][1]} VBF, {yields[1][1]} VH, {yields[2][1]} ggF')\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# cutBased_purity(ax, CutBased_events, category = 'ggF', use_weights=True)\n",
    "\n",
    "fig, axes  = plt.subplots(1,3, figsize = (10,6))\n",
    "categories = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "for i in range(3):\n",
    "    a = cutBased_purity(axes[i], CutBased_events, use_weights = True, category = categories[i])\n",
    "fig.legend(categories, ncol=4, loc = 'lower center', bbox_to_anchor=(0.5, -0.07))\n",
    "fig.suptitle('Cut based purity', x=0.5, y=0.92)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a6c3d-00c2-429f-a9cf-86d4edc9d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained_model.feature_importances_)\n",
    "plt.figure(figsize=(9,18))\n",
    "plt.figure(figsize=(9,9))\n",
    "# plot\n",
    "# c = X_train.columns\n",
    "# fields = np.unique(np.array([c[0] for c in X_train.columns]))\n",
    "\n",
    "# fields=np.array([c[0] for c in X_train.columns])\n",
    "features = trained_model.get_booster().feature_names\n",
    "importance = trained_model.feature_importances_\n",
    "# print(importance)\n",
    "# y = range(len(importance))\n",
    "\n",
    "\n",
    "fi = pd.DataFrame({'features': features, 'importance': importance})\n",
    "fi = fi.sort_values(by = 'importance', ascending=True).reset_index(drop=True)\n",
    "# print(fi['importance'])\n",
    "print(fi)\n",
    "\n",
    "n = 21 #features\n",
    "y=range(len(fi))\n",
    "plt.barh(y[-n:], fi['importance'][-n:])\n",
    "# plt.invert_yaxis()\n",
    "# plt.bar(fields, trained_model.feature_importances_)\n",
    "plt.yticks(y[-n:], labels=fi['features'][-n:])\n",
    "plt.title('Multiclass BDT avg. feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655bf57-6576-4bac-a7dc-86874fa19416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('MultiClassBDT_23Oct25.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13aa766-67ed-4b83-989e-ba29f702c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4002-114f-485e-a39f-d614e095b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import awkward as ak\n",
    "# import dask\n",
    "import json\n",
    "# from coffea import processor\n",
    "# from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplhep as hep\n",
    "# plt.style.use([hep.style.CMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb1de3-40f1-4d21-bc80-424ff333e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples.json', 'r') as file:\n",
    "    pmap = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1f226-6a08-4a43-aaea-bd5e78b0f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''processing functions'''\n",
    "def minjetkin(df):\n",
    "    # fatjets = df['ak8FatJetmsoftdrop', 'ak8FatJetPt', 'ak8FatJetEta']\n",
    "    # print(df['ak8FatJetmsoftdrop'].shape)\n",
    "    fatjet_msd = df['ak8FatJetmsoftdrop'].values[:,0]\n",
    "    fatjet_pt = df['ak8FatJetPt'].values[:,0]\n",
    "    fatjet_eta = df['ak8FatJetEta'].values[:,0]\n",
    "        # fatjets['msdcorr'] = fatjets.msoftdrop\n",
    "        # fatjets['qcdrho'] = 2 * np.log(fatjets.msdcorr / fatjets.pt)\n",
    "    candidatejet = df[\n",
    "            (fatjet_pt > 200)\n",
    "            & (abs(fatjet_eta) < 2.5)\n",
    "            # & fatjets.isTight \n",
    "        ]\n",
    "    # candidatejet = candidatejet[:, :2]\n",
    "    # candidatejet = ak.firsts(candidatejet[ak.argmax(candidatejet.particleNet_XbbVsQCD, axis=1, keepdims=True)])\n",
    "\n",
    "    # bvl = candidatejet.particleNet_XbbVsQCD\n",
    "    minjetkin=np.array([\n",
    "            (candidatejet['ak8FatJetPt'] >= 450)\n",
    "            & (candidatejet['ak8FatJetPt']< 1200)\n",
    "            & (candidatejet['ak8FatJetmsoftdrop'] >= 40.)\n",
    "            & (candidatejet['ak8FatJetmsoftdrop'] < 201.)\n",
    "            & (abs(candidatejet['ak8FatJetEta']) < 2.5)\n",
    "       ])\n",
    "    minjetkin=np.sum(minjetkin, axis=2).astype('bool').transpose()\n",
    "    # print(minjetkin.shape)\n",
    "    # print(minjetkin)\n",
    "    \n",
    "    return df[minjetkin]\n",
    "\n",
    "def get_paths(data_path, proc = 'QCD'):\n",
    "    #returns list of paths to parquet files\n",
    "    return [os.path.join(data_path, p, 'parquet') for p in pmap[proc]]\n",
    "\n",
    "def process_single(df, \n",
    "                   cuts=False,\n",
    "                   save_fields = ['weight','ak8FatJetMass'],\n",
    "                   signal = False,\n",
    "               ):    \n",
    "    if cuts: \n",
    "        dfc = minjetkin(df.copy(deep=True))\n",
    "        #add more cuts here\n",
    "    else:\n",
    "        dfc = df.copy(deep=True)\n",
    "                           \n",
    "    X = dfc[save_fields] \n",
    "\n",
    "    if signal:\n",
    "        X['isSignal']  = np.ones(X['weight'].shape[0])\n",
    "        X['category'] = [signal]*X['weight'].shape[0]\n",
    "    else: \n",
    "        X['isSignal'] = np.zeros(X['weight'].shape[0])\n",
    "        X['category'] = ['QCD']*X['weight'].shape[0]\n",
    "    del dfc\n",
    "    return X\n",
    "    \n",
    "def df2Dmatrix(X):\n",
    "    #convert final df to dmatrix for xgb\n",
    "    dmatrix = xgb.DMatrix(X, label= X['isSignal'], missing = -9999, weight = X['weight_noxsec'])\n",
    "    return dmatrix\n",
    "\n",
    "def fill_hist(process, field, cut=False, full=True):\n",
    "    \n",
    "    data_dir = '/uscms/home/bweiss/nobackup/hbb/2023' #folder containing all samples\n",
    "    #collect all field data for the process\n",
    "    dirs = get_paths(data_dir, process)\n",
    "    # print(dirs)\n",
    "    var = None\n",
    "\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)+' '+str(field)):\n",
    "        # print(d)\n",
    "        for i, file in enumerate(os.listdir(d)):\n",
    "            path = os.path.join(d,file)\n",
    "            # print(path)\n",
    "            # print(path)\n",
    "            if var is None:\n",
    "                var = get_proc_field(path, field, cut=cut)\n",
    "                # print(var.shape, var)\n",
    "            else:\n",
    "                var = np.append(var, get_proc_field(path, field, cut=cut).values, axis=0)\n",
    "                # print(var.shape, type(var))\n",
    "            # if not full:\n",
    "            #     print('stored var from only 1 parquet')\n",
    "            #     break\n",
    "    return var\n",
    "\n",
    "def accumulator(proc, isSignal=False, shallow=True): #perform data accumulation for a particular process\n",
    "    data_dir = '/uscms/home/bweiss/nobackup/hbb/2023'\n",
    "    dirs = get_paths(data_dir, proc)\n",
    "    # print(dirs)\n",
    "    data = None\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)): #runs through subsets of a process\n",
    "        # print(d)\n",
    "        for i, file in enumerate(os.listdir(d)): #runs through files in subset\n",
    "            if (i > 20) & shallow: #use only 1 parquet file from each subset if shallow\n",
    "                # print(file)\n",
    "                break\n",
    "            # elif shallow:\n",
    "            #     break\n",
    "            # print('parquet name: ', file)\n",
    "            file_path = os.path.join(d,file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            multiindex_columns = df.columns\n",
    "            # print(multiindex_columns)\n",
    "            col_string = 'ak8Fat'\n",
    "            # # save_cols = [col for col in multiindex_columns if isinstance(col, int) and col_string in col[0]]+['weight']\n",
    "            save_cols = [col for col in multiindex_columns if ( (col_string in col[0]) #save all ak8fatjet columns and weights\n",
    "                                                            and ('ass' not in col[0]) \n",
    "                                                            and ('soft' not in col[0])\n",
    "                                                              )] \n",
    "            save_cols = save_cols + [('weight', 0)] + [('weight_noxsec', 0)]\n",
    "            # if i == 0:\n",
    "                 # print('save_cols: ', save_cols)\n",
    "            \n",
    "            thisdf = process_single(df, cuts=True,\n",
    "                               save_fields = save_cols,\n",
    "                               signal = isSignal,\n",
    "                                  ) #apply cuts save select columns, add isSignal column\n",
    "            if data is None:\n",
    "                data = thisdf\n",
    "            else:\n",
    "                data = pd.concat([data, thisdf], axis = 0, ignore_index=True)\n",
    "            del thisdf\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813c623-eecf-4c56-9bbf-4afad6aba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Accumulate data, prepare it, save to mega DF '''\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning) \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def get_paths(data_path, proc = 'QCD'):\n",
    "    #returns list of paths to parquet files\n",
    "    return [os.path.join(data_path, p, 'parquet') for p in pmap[proc]]\n",
    "\n",
    "# shallow = True #take only one parquet from each process/proc subset\n",
    "\n",
    "samples = ['VBF', 'VH', 'ggF', 'QCD'] #processes to aquire\n",
    "isSignal = ['VBF', 'VH', 'ggF', False]\n",
    "\n",
    "X = None\n",
    "\n",
    "for j, s in enumerate(samples):\n",
    "    # print(s)\n",
    "    proc_data = accumulator(s, isSignal=isSignal[j], shallow=True)\n",
    "    # print(proc_data.columns)\n",
    "    if X is None:\n",
    "        X = proc_data\n",
    "        # print(X.columns)\n",
    "    else:\n",
    "        X = pd.concat([X, proc_data], axis = 0, ignore_index=True)\n",
    "    print(X['isSignal'].shape)\n",
    "    del proc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adb7f4-9235-484b-ab7d-4342302fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual omition of negative weights\n",
    "print(X.columns)\n",
    "\n",
    "print(len(X['weight_noxsec']))\n",
    "\n",
    "print(np.sum((X['weight_noxsec']<0).astype(int)))\n",
    "\n",
    "w_min = 1E-9\n",
    "mask = X['weight_noxsec']>0\n",
    "# print(mask)\n",
    "\n",
    "X = X.loc[mask[0]]\n",
    "\n",
    "# print(np.sum((X['weight_noxsec']>0).astype(int)))\n",
    "print(len(X['weight_noxsec']))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f869e38-9709-402b-a27a-9b5765d1e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting weights\n",
    "import matplotlib.pyplot as plt\n",
    "print(X['weight'][:-20])\n",
    "\n",
    "def symlog_bins(data, num_bins=10, linthresh=1.0):\n",
    "    \"\"\"\n",
    "    Generates symlog spaced bins for histograms, handling both positive and negative values.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The data for which to generate bins.\n",
    "        num_bins (int, optional): The desired number of bins. Defaults to 10.\n",
    "        linthresh (float, optional): The threshold for the linear part of the symlog scale. \n",
    "                                     Values between -linthresh and linthresh will be linearly spaced. \n",
    "                                     Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: An array of bin edges.\n",
    "    \"\"\"\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    print(min_val)\n",
    "\n",
    "    # Handle the case where data is all zeros\n",
    "    if min_val == 0 and max_val == 0:\n",
    "        return np.linspace(0, 1, num_bins + 1)\n",
    "\n",
    "    # Generate positive and negative bin edges separately\n",
    "    pos_bins = np.logspace(np.log10(linthresh), np.log10(max_val), num=num_bins // 2 + 1) if max_val > linthresh else np.array([])\n",
    "    neg_bins = -np.logspace(np.log10(linthresh), np.log10(-min_val), num=num_bins // 2 + 1) if min_val < -linthresh else np.array([])\n",
    "\n",
    "    # Include the linear part of the symlog scale\n",
    "    linear_bins = np.linspace(-linthresh, linthresh, num_bins % 2 + 1)\n",
    "\n",
    "    # Combine the bins and sort them\n",
    "    bins = np.concatenate((neg_bins, linear_bins, pos_bins))\n",
    "    bins = np.unique(bins)  # Remove duplicates in case of overlapping ranges\n",
    "    bins = np.sort(bins)\n",
    "    return bins\n",
    "\n",
    "bins =symlog_bins(X_train['weight_noxsec'],num_bins=20)\n",
    "\n",
    "\n",
    "plt.hist(X_train['weight_noxsec'], bins=bins)\n",
    "plt.hist(X_test['weight_noxsec'], bins=bins)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('symlog')\n",
    "# plt.xlim([]\n",
    "# print(X.columns)\n",
    "\n",
    "DM_train = df2Dmatrix(X_train)\n",
    "DM_test = df2Dmatrix(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325039c-c69e-475b-b81c-41b07eb0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BDT model\n",
    "# import xgboost as xgb\n",
    "\n",
    "# see https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\n",
    "# for detailed explanations of parameters\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # number of boosting rounds (i.e. number of decision trees)\n",
    "    max_depth=3,  # max depth of each decision tree\n",
    "    learning_rate=0.1,\n",
    "    early_stopping_rounds=20,  # how many rounds to wait to see if the loss is going down\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097af18-47bc-49d5-b60e-2fcac372ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_cols = ['isSignal', 'weight', 'weight_noxsec','category']\n",
    "trained_model = model.fit(\n",
    "    X_train.drop(omit_cols, axis=1), #data should not include label column OR weights\n",
    "    X_train['isSignal'], #labels\n",
    "    # Y_train_val,\n",
    "    # xgboost uses the last set for early stopping\n",
    "    # https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping\n",
    "    eval_set=[(X_train.drop(omit_cols, axis=1), X_train['isSignal']), \n",
    "              (X_test.drop(omit_cols, axis=1), X_test['isSignal'])],  # sets for which to save the loss\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59e8e8-ea47-4a2b-bc95-c6532e6f07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = trained_model.evals_result()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "for i, label in enumerate([\"Train\", \"Test\"]):\n",
    "    plt.plot(evals_result[f\"validation_{i}\"][\"logloss\"], label=label, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot ROC\n",
    "Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(X_test['isSignal'], Y_predict[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "ax.plot(fpr, tpr, lw=2, color=\"cyan\", label=\"auc = %.3f\" % (roc_auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"random chance\")\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel(\"false positive rate\")\n",
    "ax.set_ylabel(\"true positive rate\")\n",
    "ax.set_title(\"receiver operating curve\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343f573-b205-4cff-b923-49d0f5d99b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (5,4))\n",
    "samples = ['QCD', 'ggF', 'VBF', 'VH', ]\n",
    "colors = ['black', [0.6,1,0.6], [0,0.8,0], [0,0.3,0]]\n",
    "print(Y_predict[:, 1].shape)\n",
    "print(X_test['weight'].shape)\n",
    "\n",
    "bins = np.linspace(0,1,10)\n",
    "signals = pd.DataFrame()\n",
    "for i, s in enumerate(samples[1:]):\n",
    "    category_mask = X_test['category'] == s\n",
    "    w = X_test['weight'][category_mask][0]\n",
    "    data = pd.DataFrame(Y_predict[category_mask, 1], columns=[s])\n",
    "    signals = pd.concat([signals, data], axis=1)\n",
    "    # if s is not 'QCD':\n",
    "    #     # ax.hist(Y_predict[category_mask, 1], color = colors[i], label = samples[i], stacked=True, linewidth=3, bins=bins #weights = w\n",
    "    #        )\n",
    "    # else:\n",
    "    #     ax.hist(Y_predict[category_mask, 1], histtype='step', color = colors[i], label = samples[i], stacked=False, linewidth=3, bins=bins #weights = w\n",
    "    #        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "QCD_mask =  X_test['category'] == 'QCD'\n",
    "ax.hist(Y_predict[QCD_mask, 1], histtype='step', color = colors[0], label = samples[0], stacked=False, linewidth=3, bins=bins #weights = w\n",
    "           )\n",
    "ax.hist(signals, #color = colors[1:],\n",
    "        label =signals.columns, stacked=True, linewidth=3, bins=bins #weights = w\n",
    "           )\n",
    "    \n",
    "ax.set_yscale('log')\n",
    "ax.legend(samples)\n",
    "ax.set_ylabel('Events')\n",
    "ax.set_xlabel('BDT score')\n",
    "ax.set_title('S/B classifer scores by channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae977c-be53-402f-b089-8e2dbc4f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a6c3d-00c2-429f-a9cf-86d4edc9d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained_model.feature_importances_)\n",
    "plt.figure(figsize=(9,18))\n",
    "# plot\n",
    "# c = X_train.columns\n",
    "# fields = np.unique(np.array([c[0] for c in X_train.columns]))\n",
    "\n",
    "# fields=np.array([c[0] for c in X_train.columns])\n",
    "features = trained_model.get_booster().feature_names\n",
    "importance = trained_model.feature_importances_\n",
    "# print(importance)\n",
    "# y = range(len(importance))\n",
    "y=range(10)\n",
    "\n",
    "fi = pd.DataFrame({'features': features, 'importance': importance})\n",
    "fi.sort_values(by = 'importance', ascending=False)\n",
    "print(fi['importance'])\n",
    "print(fi)\n",
    "\n",
    "\n",
    "plt.barh(y, fi['importance'].loc[:9])\n",
    "# plt.invert_yaxis()\n",
    "# plt.bar(fields, trained_model.feature_importances_)\n",
    "plt.yticks(y, labels=fi['features'].loc[:9])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948a1d8-a7d8-46e5-9fab-dc0dc017e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

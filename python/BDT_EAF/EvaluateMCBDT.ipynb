{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508818c-806b-4c62-bbb2-8d8c0dcee53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import awkward as ak\n",
    "# import dask\n",
    "import json\n",
    "# from coffea import processor\n",
    "# from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import hist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.CMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6b199-358a-4388-8464-ef0964308d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('samples_noQCD2000.json', 'r') as file:\n",
    "with open('samples.json', 'r') as file:\n",
    "    pmap = json.load(file)\n",
    "\n",
    "print(pmap.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95856e01-6582-4d63-9653-40c9f67dd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''processing functions'''\n",
    "'''Cut definitions'''\n",
    "def minjetkin(df):\n",
    "    # fatjets = df['ak8FatJetmsoftdrop', 'ak8FatJetPt', 'ak8FatJetEta']\n",
    "    # print(df['ak8FatJetmsoftdrop'].shape)\n",
    "    fatjet_msd = df['FatJet0_msd'].values\n",
    "    fatjet_pt = df['FatJet0_pt'].values\n",
    "    fatjet_eta = df['FatJet0_eta'].values\n",
    "        # fatjets['msdcorr'] = fatjets.msoftdrop\n",
    "        # fatjets['qcdrho'] = 2 * np.log(fatjets.msdcorr / fatjets.pt)\n",
    "    candidatejet = df[\n",
    "            (fatjet_pt > 200)\n",
    "            & (abs(fatjet_eta) < 2.5)\n",
    "            # & fatjets.isTight \n",
    "        ]\n",
    "\n",
    "    # candidatejet = candidatejet[:, :2]\n",
    "    # candidatejet = ak.firsts(candidatejet[ak.argmax(candidatejet.particleNet_XbbVsQCD, axis=1, keepdims=True)])\n",
    "\n",
    "    # bvl = candidatejet.particleNet_XbbVsQCD\n",
    "    minjetkin=np.array([\n",
    "            (candidatejet['FatJet0_pt'] >= 450)\n",
    "            & (candidatejet['FatJet0_pt']< 1200)\n",
    "            & (candidatejet['FatJet0_msd'] >= 40.)\n",
    "            & (candidatejet['FatJet0_msd'] < 201.)\n",
    "            & (abs(candidatejet['FatJet0_eta']) < 2.5)\n",
    "       ])\n",
    "    # minjetkin=np.sum(minjetkin, axis=1).astype('bool').transpose()\n",
    "    minjetkin = minjetkin.astype('bool').transpose()\n",
    "\n",
    "    # print(minjetkin)\n",
    "    # print(minjetkin.shape)\n",
    "    # print(minjetkin)\n",
    "    \n",
    "    return df[minjetkin]\n",
    "    \n",
    "def get_paths(year, data_path, proc = 'QCD', deep=False):\n",
    "    #returns list of paths to parquet files\n",
    "    parquet_parents = [os.path.join(data_path, year, p, 'parquet','signal-all') for p in pmap[proc]]\n",
    "    \n",
    "    if deep:\n",
    "        file_list=None\n",
    "        for parent in parquet_parents:\n",
    "            if file_list is None:\n",
    "                file_list = [os.path.join(parent,file)for file in os.listdir(parent)]\n",
    "            else:\n",
    "                file_list = np.append(file_list, [os.path.join(parent,file)for file in os.listdir(parent)])\n",
    "    else:\n",
    "        file_list=parquet_parents\n",
    "    return file_list\n",
    "\n",
    "def mode2category(mode):\n",
    "    cats = np.array(['ggF', 'VBF', 'VH'])\n",
    "    if mode not in cats:\n",
    "        raise ValueError(f'Decay mode {mode} not in {cats}')\n",
    "    category = (cats==mode).astype(int)\n",
    "    return category\n",
    "\n",
    "print(mode2category('VBF'))\n",
    "        \n",
    "    \n",
    "    \n",
    "def process_single(df, \n",
    "                   cuts=False,\n",
    "                   save_fields = ['weight','FatJet0_pt'],\n",
    "                   signal = False,\n",
    "                   category = 'QCD', #category order: ['ggF', 'VBF', 'VH']\n",
    "               ):    \n",
    "    if cuts: \n",
    "        dfc = minjetkin(df.copy(deep=True))\n",
    "        #add more cuts here\n",
    "    else:\n",
    "        dfc = df.copy(deep=True)\n",
    "                           \n",
    "    X = dfc[save_fields] \n",
    "\n",
    "    if signal:\n",
    "        X['isSignal']  = np.ones(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = [signal]*X['weight'].shape[0]\n",
    "        # X['y'] = mode2category(X['category'])*X['weight'].shape[0]\n",
    "    else: \n",
    "        X['isSignal'] = np.zeros(X['weight'].shape[0]).astype(int)\n",
    "        X['category'] = ['QCD']*X['weight'].shape[0]\n",
    "        # X['y'] = np.array([0,0,0]*X['weight'].shape[0])\n",
    "    del dfc\n",
    "    return X\n",
    "\n",
    "def get_sum_genweights(data_dir: Path, dataset: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the sum of genweights for a given dataset.\n",
    "    :param data_dir: The directory where the datasets are stored.\n",
    "    :param dataset: The name of the dataset to get the genweights for.\n",
    "    :return: The sum of genweights for the dataset.\n",
    "    \"\"\"\n",
    "    total_sumw = 0\n",
    "    try:\n",
    "        # Load the genweights from the pickle file\n",
    "        for pickle_file in list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")):\n",
    "            with Path(pickle_file).open(\"rb\") as file:\n",
    "                out_dict = pickle.load(file)\n",
    "            # The sum of weights is stored in the \"sumw\" key\n",
    "            # You can access it like this:\n",
    "            for key in out_dict:\n",
    "                sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "            total_sumw += sumw\n",
    "        print(pickle_file)\n",
    "    except:\n",
    "        print(\"shit: \", list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\"))[0])\n",
    "        warnings.warn(\n",
    "            f\"Error loading genweights for dataset: {dataset}. Skipping.\",\n",
    "            category=UserWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        total_sumw = 1\n",
    "\n",
    "    # print(f\"Total sum of weights for all pickles for {dataset}: {total_sumw}\")\n",
    "    return total_sumw\n",
    "\n",
    "def accumulator(proc, isSignal=False, shallow=False, path=None): #perform data accumulation for a particular process\n",
    "    if path is None:\n",
    "        data_dir = '/uscms/home/bweiss/nobackup/hbb/'\n",
    "        dirs = get_paths('2023', data_dir, proc)\n",
    "        # print(dirs)\n",
    "    else:\n",
    "        if os.path.isfile(path):\n",
    "            dirs = [path]\n",
    "        else: \n",
    "            dirs = os.listdir(path)\n",
    "    all_data = None\n",
    "    for d in tqdm(dirs, desc=\"Processing \"+str(proc)): #runs through subsets of a process\n",
    "        dataset = None\n",
    "        if os.path.isfile(d):\n",
    "            ds = [d]\n",
    "        else:\n",
    "            ds = os.listdir(d)\n",
    "        # print(ds)\n",
    "        for i, file in enumerate(ds): #runs through files in subset\n",
    "            if shallow and i>shallow: #use only 1 parquet file from each subset if shallow\n",
    "                print(file)\n",
    "                break\n",
    "            file_path = os.path.join(d,file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            cols = df.columns\n",
    "            excluded_cols = ['MET', #'FatJet0_pt', 'FatJet0_msd', 'FatJet0_pnetMass', 'FatJet0_pnetTXbb'\n",
    "                            ]\n",
    "            save_cols = [c for c in cols if (c not in excluded_cols) \n",
    "                         and ('Gen' not in c)\n",
    "                         \n",
    "                        ]\n",
    "            thisdf = process_single(df, cuts=True,\n",
    "                               save_fields = save_cols,\n",
    "                               signal = isSignal,\n",
    "                                  ) #apply cuts save select columns, add isSignal column\n",
    "            if dataset is None:\n",
    "                dataset = thisdf\n",
    "            else:\n",
    "                dataset = pd.concat([dataset, thisdf], axis = 0, ignore_index=True)\n",
    "            del thisdf\n",
    "        #reweight events but sum of weights in a MC dataset\n",
    "        this_dataset = Path(d).parent.parent.name\n",
    "        print(this_dataset)\n",
    "        sumW = get_sum_genweights(Path('/uscms/home/bweiss/nobackup/hbb/2023'), this_dataset)\n",
    "        dataset['sumW'] = np.ones_like(dataset['weight'])*sumW\n",
    "        # sumW = np.sum(dataset['weight'].values)\n",
    "        dataset['weight_final'] = abs(dataset['weight'])/sumW\n",
    "        print(f'sum of all weights in {d} is {sumW}')\n",
    "        dataset['MC_name'] = this_dataset\n",
    "        if all_data is None:\n",
    "            all_data = dataset\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, dataset], axis = 0, ignore_index=True)\n",
    "        # del dataset\n",
    "            \n",
    "    # print('save_cols: ', save_cols)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa71686-8874-4bf3-95ae-f026d155711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.load_model('MultiClassBDT_23Oct25.json')\n",
    "omit_cols = ['isSignal', 'weight','weight_final', \n",
    "             'category', 'FatJet0_pt', 'FatJet0_msd', 'FatJet0_msdmatched',\n",
    "             'MC_name', 'sumW', 'genWeight',\n",
    "             'FatJet0_pnetMass', 'FatJet0_pnetTXbb', 'FatJet0_pnetTXgg',\n",
    "             'FatJet0_pnetTXcc', 'FatJet0_pnetTXqq', 'FatJet0_pnetXbbXcc', 'FatJet0_pnetTQCD',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85b365-1cc7-417a-aef0-fa2300adfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Accumulate data, BDT predict it, store BDT score and some other vars'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "MC_cats = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "MC_cats += ['QCD','Wto2Q','Zto2Q','VV','TopFS']\n",
    "\n",
    "# samples = ['QCD']\n",
    "# isSignal = ['QCD']\n",
    "\n",
    "for j, s in enumerate(MC_cats):\n",
    "    Y = pd.DataFrame()\n",
    "    X = accumulator(s, isSignal=s, shallow=False)\n",
    "    _, X_test = train_test_split(X, test_size=0.2, random_state=42, shuffle=True, #stratify = X['isSignal']\n",
    "                                 )\n",
    "    X_test.reset_index()\n",
    "    # print(X_test.columns)\n",
    "    # X_toBDT = X_test.drop(omit_cols, axis=1)\n",
    "    # print(X_test.columns)\n",
    "    Y_predict = model.predict_proba(X_test.drop(omit_cols, axis=1))\n",
    "    print(Y_predict)\n",
    "    # Y['BDT_score'] = Y_predict\n",
    "    Y['BDT_cat'] = np.argmax(Y_predict, axis =1)\n",
    "    Y['MC_cat'] = s\n",
    "    Y['MC_name'] = X_test['MC_name'].values\n",
    "    Y['weight_final'] = X_test['weight_final'].values\n",
    "    Y['FatJet0_msd'] = X_test['FatJet0_msd'].values\n",
    "    Y.to_parquet(f'./BDT_predictions/{s}.parquet')\n",
    "\n",
    "print(Y)\n",
    "print(X_test['MC_name'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c8483-866b-4f4f-afa2-ab18d6b01ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97d305-0595-4a14-88bb-eaf9174b70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Compute yield from list of weights'''\n",
    "def get_yield(y, BDT_cat = 'VH', MC_cat = 'TopFS'):\n",
    "    BDT_cat_dict = {'VBF':0, 'VH':1, 'ggF':2}\n",
    "    BDT_class = BDT_cat_dict[BDT_cat]\n",
    "    BDT_mask = y['BDT_cat'] == BDT_class\n",
    "    MC_mask = y['MC_cat'] == MC_cat\n",
    "\n",
    "    y_mask = BDT_mask&MC_mask\n",
    "    # print(BDT_mask)\n",
    "    # print(MC_mask)\n",
    "    # print(y_mask)\n",
    "    yeeld = np.sum(y[y_mask]['weight_final'])\n",
    "    N = np.sum(y_mask)\n",
    "    return yeeld, N\n",
    "\n",
    "# print(get_yield(Y, BDT_cat = 'VH', MC_cat = 'TopFS'))\n",
    "MC_cats = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "MC_cats += ['QCD','Wto2Q','Zto2Q','VV','TopFS']\n",
    "\n",
    "pred_path = './BDT_predictions'\n",
    "yield_table = pd.DataFrame(index = list(BDT_cat_dict.keys()))\n",
    "for df_file in os.listdir(pred_path):\n",
    "    df = pd.read_parquet(os.path.join(pred_path, df_file))\n",
    "    MC_cat = df['MC_cat'][0]\n",
    "    print(MC_cat)\n",
    "    cat_yields = np.zeros(3)\n",
    "    for i, cat in enumerate(BDT_cat_dict.keys()):\n",
    "        cat_yields[i] = get_yield(df, BDT_cat = cat, MC_cat = MC_cat)[0]\n",
    "    yield_table[MC_cat] = cat_yields\n",
    "print(yield_table)\n",
    "yield_table.to_csv('yield_table.csv')\n",
    "    # cat_index = le.transform([category])\n",
    "        \n",
    "    # pred_mask = X['CB_cat'] == category\n",
    "    # X_pred = X[pred_mask]\n",
    "    # use_weights = int(use_weights)\n",
    "    # yields = np.zeros((4,2))\n",
    "    # bottom = 0\n",
    "    # for cat in range(4):\n",
    "    #     truecat = le.inverse_transform([cat])[0]\n",
    "    #     cat_mask = X_pred['true_cat'] == truecat\n",
    "    #     yields[cat][:] = [np.sum(cat_mask), \n",
    "    #                       np.sum(X_pred['weight_final'][cat_mask].values, axis = 0)]\n",
    "    # sumw = np.sum(yields[:,1])\n",
    "    # yields[:,1] = yields[:,1]/sumw\n",
    "    # for c in range(4):\n",
    "    # print(\"----------------------------------------\")\n",
    "    # print(f'Predicted {category} purity: {yields[cat_index,1]})')\n",
    "    # print(f'contains {yields[0][0]} VBF, {yields[1][0]} VH, {yields[2][0]} ggF')\n",
    "    # print(f'with yields: {yields[0][1]} VBF, {yields[1][1]} VH, {yields[2][1]} ggF')\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# cutBased_purity(ax, CutBased_events, category = 'ggF', use_weights=True)\n",
    "\n",
    "# fig, axes  = plt.subplots(1,3, figsize = (10,6))\n",
    "# categories = ['VBF', 'VH', 'ggF', 'ttH']\n",
    "# for i in range(3):\n",
    "#     a = cutBased_purity(axes[i], CutBased_events, use_weights = True, category = categories[i])\n",
    "# fig.legend(categories, ncol=4, loc = 'lower center', bbox_to_anchor=(0.5, -0.07))\n",
    "# fig.suptitle('Cut based purity', x=0.5, y=0.92)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb49ced-9af7-49ee-85ae-553e355dd3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
